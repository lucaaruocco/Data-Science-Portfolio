# Load the necessary libraries
install.packages("adabag")
library(adabag)
library(ConsRank)
library(fastDummies)
library(dplyr)
library(rpart)
library(rpart.plot)
library(fastDummies)

# The datasets GermanElections2005 contains socio-economic information from regions of Germany 
# and their respective electoral results. The 413 records correspond to the administrative 
# districts of Germany, which are described by 39 covariates, such as age and education 
# of the population, economic indicators (e.g. GDP growth, percentage of unemployment), 
# indicators of the labour workforce in different sectors such as production, public service, etc.
# The outcome variable consists of transformed election results of the five major political 
# parties into rankings: 
# CDU (conservative), SPD (centre-left), FDP (liberal),
# Green (centre-left), and Left (left-wing).

# Load the dataset
load("german2005.RData")
storage.mode(y) <- "integer"

# Convert categorical variables 'State' and 'Region' into dummy/indicator variables
x_new <- dummy_cols(x, select_columns = c("State", "Region"), remove_selected_columns = TRUE)

# Prepare the data with item weights (equal weights in this case)
dati <- prep_data(y, x_new, iw = rep(1, 5))

# Divide the data into training and test sets
set.seed(12345)
l <- nrow(dati)
sub <- sample(1:l, l * 2 / 3)

data_train <- dati[sub, ]
data_test1 <- dati[-sub, ]

y_train <- y[sub, ]
y_test <- y[-sub, ]


# Apply ensemble ranking using AdaBoost.M1
Boosting_German <- Ensemble_ranking_IW(
  Label ~ .,
  data = data_train,
  iw = c(1, 1, 1, 1, 1),
  mfinal = 50,
  control = rpart.control(maxdepth = 4, cp = -1),
  algo = "boosting"
)



# Arguments of rpart.control

# maxdepth
# Set the maximum depth of any node of the final tree, with the root node counted as depth 0. Values greater
# than 30 rpart will give nonsense results on 32-bit machines.

#cp
# complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not
# attempted.

# minsplit
# the minimum number of observations that must exist in a node in order for a split to be attempted.

# minbucket 
#the minimum number of observations in any terminal node.

# maxcompete
# the number of competitor splits retained in the output. It is useful to know not just which split was chosen,
# but which variable came in second, third, etc.

# maxsurrogate
# the number of surrogate splits retained in the output.

# usesurrogate
# how to use surrogates in the splitting process. 0 means display only; an observation with a missing value for
# the primary split rule is not sent further down the tree. 1 means use surrogates, in order, to split subjects
# missing the primary variable.

# xval
# number of cross-validations.



# Visualize the first and last decision trees generated by the boosting algorithm
x11()
rpart.plot(Boosting_German$trees[[1]])
x11()
rpart.plot(Boosting_German$trees[[50]])

# Check variable importance in the Boosting model
Boosting_German$importance

# Apply ensemble ranking using Bagging
bagging_German <- Ensemble_ranking_IW(
  Label ~ .,
  data = data_train,
  iw = c(1, 1, 1, 1, 1),
  mfinal = 50,
  control = rpart.control(maxdepth = 4, cp = -1),
  algo = "bagging"
)

# Check variable importance in the Bagging model
bagging_German$importance

# Visualize the first and last decision trees generated by the bagging algorithm
x11()
rpart.plot(bagging_German$trees[[1]])
x11()
rpart.plot(bagging_German$trees[[50]])

# Apply a single decision tree for comparison
Single_tree_German <- Ensemble_ranking_IW(
  Label ~ .,
  data = data_train,
  iw = c(1, 1, 1, 1, 1),
  mfinal = 1,
  control = rpart.control(maxdepth = 4, cp = -1),
  algo = "bagging"
)

# Check variable importance in the single decision tree model
Single_tree_German$importance

x11()
rpart.plot(Single_tree_German$trees[[1]])


Single_tree_German_maxd2 <- Ensemble_ranking_IW(
  Label ~ .,
  data = data_train,
  iw = c(1, 1, 1, 1, 1),
  mfinal = 1,
  control = rpart.control(maxdepth = 2, cp = -1),
  algo = "bagging"
)


x11()
rpart.plot(Single_tree_German_maxd2$trees[[1]])




# Combine the importance metrics from Boosting, Bagging, and Single Tree models
df <- data.frame(
  Variable = rep(names(Boosting_German$importance), 3),
  Importance = c(Boosting_German$importance, bagging_German$importance, Single_tree_German$importance),
  Model = rep(c("Boosting", "Bagging", "Single Tree"), each = length(colnames(x_new)))
)

# Plot the variable importance for the three models using ggplot2
ggplot(df, aes(x = reorder(Variable, -Importance), y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Variable importance in Bagging, Boosting and Single Tree",
       x = "Variable",
       y = "Importance") +
  theme_minimal() +
  theme(legend.position = "top")

# Filter the data to show only variables with positive importance and plot
df |> 
  dplyr::filter(Importance > 0) |> 
  ggplot(aes(x = reorder(Variable, -Importance), y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Variable importance in Bagging, Boosting and Single Tree",
       x = "Variable",
       y = "Importance") +
  theme_minimal() +
  theme(legend.position = "top")

# Function to predict rankings using boosting ensemble model
predict_boosting <- function(object, newdata, ntree) {
  # Extract necessary components from the object
  item <- object$args$item
  tab <- object$args$perm_tab_complete_up
  
  # Initialize the prediction matrix
  pred <- matrix(0, ncol = ntree, nrow = dim(newdata)[1])
  
  # Populate the prediction matrix with predictions from each tree
  for (j in 1:ntree) {
    pred[, j] <- predict(object$trees[[j]], newdata = newdata)
  }
  
  # Determine weights for the final prediction
  if (!inherits(object, "boosting")) {
    ponderacion <- rep(1, length(object$trees))
  } else {
    ponderacion <- object$weights
  }
  
  # Initialize the final prediction matrix
  final_pred <- matrix(NA, nrow = dim(newdata)[1], ncol = item)
  
  # Calculate the final predictions
  for (i in 1:dim(newdata)[1]) {
    final_pred[i, ] <- suppressMessages(consrank(left_join(tibble(Label = pred[i, ]), tab)[, -1], wk = ponderacion)$Consensus[1, ])
    print(i)
  }
  # Return the final predictions
  final_pred
}

# Generate predictions using the first 10 trees in the boosting model
Example <- predict_boosting(Boosting_German, newdata = data_train, ntree = 10)

# Function to compare predictions with actual data using Tau_x metric
compare.error <- function(real, predicted) {
  bos = rep(NA, nrow(real))
  for (i in 1:nrow(real)) {
    bos[i] <- tau_x(real[i, ], predicted[i, ])
  }
  bos
}

# Compare errors between actual rankings and predicted rankings for the training set
compare.error(real = y_train, predicted = Example)
summary(compare.error(real = y_train, predicted = Example))

# Define a sequence of tree counts for evaluation
try <- c(1, seq(5, 50, by = 5))

# Initialize vectors to store Tau_x values for boosting and bagging models
tau_boost <- rep(NA, length(try))
tau_bag <- rep(NA, length(try))

# Evaluate and compare the models for different tree counts
for (i in 1:length(try)) {
  Bos <- predict_boosting(Boosting_German, data_test1, ntree = try[i])
  Bag <- predict_boosting(bagging_German, data_test1, ntree = try[i])
  
  tau_boost[i] <- mean(compare.error(y_test, Bos))
  tau_bag[i] <- mean(compare.error(y_test, Bag))
}

# Create a dataframe to store the errors for plotting
df_errors <- data.frame(
  Iteration = rep(try, 2),
  Tau = c(tau_boost, tau_bag),
  Error = c((1 - (tau_boost + 1) / 2), 1 - (tau_bag + 1) / 2),
  Model = rep(c("Boosting", "Bagging"), each = length(try))
)

# Plot the errors across different iterations using ggplot2
ggplot(df_errors, aes(x = Iteration, y = Error, color = Model, group = Model)) +
  geom_line() +
  geom_point() +
  labs(title = "Errors of Boosting and Bagging Models over 50 Iterations",
       x = "Iteration",
       y = "Error",
       color = "Model") +
  theme_minimal() +
  theme(legend.position = "top")


# Generate predictions using the first 30 trees in the boosting and bagging models
Bos_30 <- predict_boosting(Boosting_German, data_test1, ntree = 30)
Bag_30 <- predict_boosting(bagging_German, data_test1, ntree = 30)

# Identify rural and urban areas in the test set
Id_Rural <- which(data_test1$Type == "Rural")
Id_Urban <- which(data_test1$Type == "Urban")

# Compare consensus rankings for rural areas between actual and predicted data
consrank(y_test[Id_Rural, ])
consrank(Bos_30[Id_Rural, ])
consrank(Bag_30[Id_Rural, ])

# Compare consensus rankings for urban areas between actual and predicted data
consrank(y_test[Id_Urban, ])
consrank(Bos_30[Id_Urban, ])
consrank(Bag_30[Id_Urban, ])


# Calculate the first and third quartiles of income
qs <- quantile(data_test1$Income)[c(2,4)]

# Identify indices of individuals with income in the 'Poor' and 'Rich' categories
Poor <- which(data_test1$Income <= qs[1])  # Individuals with income less than or equal to the first quartile
Rich <- which(data_test1$Income >= qs[2])  # Individuals with income greater than or equal to the third quartile

# Calculate and display the consensus ranking for 'Poor' individuals
consrank(y_test[Poor,])            # Actual rankings for 'Poor' individuals
consrank(Bos_30[Poor,])            # Boosting model predictions using 30 trees for 'Poor' individuals
consrank(Bag_30[Poor,])            # Bagging model predictions using 30 trees for 'Poor' individuals

# Calculate and display the mean error for the Boosting and Bagging models for 'Poor' individuals
mean(compare.error(y_test[Poor,], Bos_30[Poor,]))  # Mean error for Boosting model (30 trees) for 'Poor' individuals
mean(compare.error(y_test[Poor,], Bag_30[Poor,]))  # Mean error for Bagging model (30 trees) for 'Poor' individuals

# Calculate and display the consensus ranking for 'Rich' individuals
consrank(y_test[Rich,])            # Actual rankings for 'Rich' individuals
consrank(Bos_30[Rich,])            # Boosting model predictions using 30 trees for 'Rich' individuals
consrank(Bag_30[Rich,])            # Bagging model predictions using 30 trees for 'Rich' individuals

# Calculate and display the mean error for the Boosting and Bagging models for 'Rich' individuals
mean(compare.error(y_test[Rich,], Bos_30[Rich,]))  # Mean error for Boosting model (30 trees) for 'Rich' individuals
mean(compare.error(y_test[Rich,], Bag_30[Rich,]))  # Mean error for Bagging model (30 trees) for 'Rich' individuals


# save.image(file = "g2005_full.Rdata")


##############################################################
# Simulations

# Load the simulated dataset
load(file = "Simulated.Rdata")

#load("kfold_SD.Rdata")
# Prepare the data with item weights
dati <- prep_data(y, x, iw = rep(1, 4))

# Split the data into training and test sets
set.seed(54321)
samp <- sample(nrow(dati))
l <- length(dati[, 1])
sub <- sample(1:l, 2 * l / 3)
data_train <- dati[sub, ]
data_test1 <- dati[-sub, ]

y_train <- y[sub, ]
y_test <- y[-sub, ]

# Train a boosting model using the ensemble ranking method
Boosting_Simulated <- Ensemble_ranking_IW(
  Label ~ .,
  data = data_train,
  iw = c(1, 1, 1, 1),
  mfinal = 50,
  control = rpart.control(maxdepth = 4, cp = -1),
  algo = "boosting"
)

# Train a bagging model using the ensemble ranking method
bagging_Simulated <- Ensemble_ranking_IW(
  Label ~ .,
  data = data_train,
  iw = c(1, 1, 1, 1),
  mfinal = 50,
  control = rpart.control(maxdepth = 4, cp = -1),
  algo = "bagging"
)

# Train a single decision tree model for comparison
Stree <- Ensemble_ranking_IW(
  Label ~ .,
  data = data_train,
  iw = c(1, 1, 1, 1),
  mfinal = 1,
  control = rpart.control(maxdepth = 4, cp = -1),
  algo = "bagging"
)

# Create a data frame for variable importance across different models
df <- data.frame(
  Variable = rep(names(Boosting_Simulated$importance), 3),
  Importance = c(Boosting_Simulated$importance, bagging_Simulated$importance, Stree$importance),
  Model = rep(c("Boosting", "Bagging", "Single Tree"), each = length(colnames(x)))
)

# Plot variable importance using ggplot2
ggplot(df, aes(x = reorder(Variable, -Importance), y = Importance, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_flip() +
  labs(title = "Variable Importance in Bagging, Boosting, and Single Tree Models",
       x = "Variables",
       y = "Importance") +
  theme_minimal() +
  theme(legend.position = "top")

# Function to predict rankings using a specified number of trees in a boosting model
predict_boosting <- function(object, newdata, ntree) {
  # Extract necessary components from the object
  item <- object$args$item
  tab <- object$args$perm_tab_complete_up
  
  # Initialize the prediction matrix
  pred <- matrix(0, ncol = ntree, nrow = dim(newdata)[1])
  
  # Populate the prediction matrix with predictions from each tree
  for (j in 1:ntree) {
    pred[, j] <- predict(object$trees[[j]], newdata = newdata)
  }
  
  # Determine weights for the final prediction
  if (!inherits(object, "boosting")) {
    ponderacion <- rep(1, length(object$trees))
  } else {
    ponderacion <- object$weights
  }
  
  # Initialize the final prediction matrix
  final_pred <- matrix(NA, nrow = dim(newdata)[1], ncol = item)
  
  # Calculate the final predictions
  for (i in 1:dim(newdata)[1]) {
    final_pred[i, ] <- suppressMessages(consrank(left_join(tibble(Label = pred[i, ]), tab)[, -1], wk = ponderacion)$Consensus[1,])
    print(i)
  }
  # Return the final predictions
  final_pred
}

# Function to calculate the error between actual and predicted rankings
compare.error <- function(real, predicted) {
  bos = rep(NA, nrow(real))
  for (i in 1:nrow(real)) {
    bos[i] <- tau_x(real[i, ], predicted[i, ])
  }
  bos
}

# Define the iterations to test in boosting and bagging models
try <- c(1, seq(5, 50, by = 5))

# Initialize vectors to store errors for boosting and bagging
tau_boost <- rep(NA, length(try))
tau_bag <- rep(NA, length(try))

# Calculate and store errors for each iteration of the models
for (i in 1:length(try)) {
  Bos <- predict_boosting(Boosting_Simulated, data_test1, ntree = try[i])
  Bag <- predict_boosting(bagging_Simulated, data_test1, ntree = try[i])
  
  tau_boost[i] <- mean(compare.error(y_test, Bos))
  tau_bag[i] <- mean(compare.error(y_test, Bag))
}

# Create a data frame to store errors across iterations for both models
df_errors <- data.frame(
  Iteration = rep(try, 2),
  Tau = c(tau_boost, tau_bag),
  Error = c((1 - (tau_boost + 1) / 2), 1 - (tau_bag + 1) / 2),
  Model = rep(c("Boosting", "Bagging"), each = length(try))
)

# Plot errors across iterations using ggplot2
ggplot(df_errors, aes(x = Iteration, y = Error, color = Model, group = Model)) +
  geom_line() +
  geom_point() +
  labs(title = "Errors in Boosting and Bagging Models Over 50 Iterations",
       x = "Iteration",
       y = "Error",
       color = "Model") +
  theme_minimal() +
  theme(legend.position = "top")


######################################
######################################
######################################
################################################################################ 
# K-fold cross-validation

# Shuffle the data and prepare for k-fold cross-validation
set.seed(54321)
shuffle <- sample(nrow(x))
x_shuf <- x[shuffle, ]
y_shuf <- y[shuffle, ]

dati <- prep_data(y_shuf, x_shuf, iw = rep(1, 4))
folds <- cut(seq(1, nrow(dati)), breaks = 5, labels = FALSE)

# Initialize lists to store errors for each fold in boosting and bagging
tau_boost_K <- list()
tau_bag_K <- list()

# Perform 5-fold cross-validation
for (K in 1:5) {
  
  # Split data into training and test sets for the Kth fold
  data_train <- dati[folds != K, ]
  data_test1 <- dati[folds == K, ]
  
  y_train <- y_shuf[folds != K, ]
  y_test <- y_shuf[folds == K, ]
  
  # Train boosting model on the training data
  Boosting_German <- Ensemble_ranking_IW(
    Label ~ .,
    data = data_train,
    iw = c(1, 1, 1, 1),
    mfinal = 50,
    control = rpart.control(maxdepth = 4, cp = -1),
    algo = "boosting"
  )
  
  # Train bagging model on the training data
  bagging_German <- Ensemble_ranking_IW(
    Label ~ .,
    data = data_train,
    iw = c(1, 1, 1, 1),
    mfinal = 50,
    control = rpart.control(maxdepth = 4, cp = -1),
    algo = "bagging"
  )
  
  # Initialize vectors to store errors for each iteration
  tau_boost <- rep(NA, length(try))
  tau_bag <- rep(NA, length(try))
  
  # Calculate and store errors for each iteration of the models
  for (i in 1:length(try)) {
    Bos <- predict_boosting(Boosting_German, data_test1, ntree = try[i])
    Bag <- predict_boosting(bagging_German, data_test1, ntree = try[i])
    tau_boost[i] <- mean(compare.error(y_test, Bos))
    tau_bag[i] <- mean(compare.error(y_test, Bag))
  }  
  tau_boost_K[[K]] <- tau_boost
  tau_bag_K[[K]] <- tau_bag
  
  cat("fold", K)
}

# Aggregate and calculate mean errors across all folds for boosting and bagging
final_bag <- as.data.frame(do.call(cbind, tau_bag_K))
final_boost <- as.data.frame(do.call(cbind, tau_boost_K))

apply(final_bag, 1, mean)

# Create a data frame to store errors across iterations for both models after cross-validation
df_errors_kfold <- data.frame(
  Iteration = rep(try, 2),
  Tau = c(apply(final_boost, 1, mean), apply(final_bag, 1, mean)),
  Error = c((1 - (apply(final_boost, 1, mean) + 1) / 2), 1 - (apply(final_bag, 1, mean) / 2)),
  Model = rep(c("Boosting", "Bagging"), each = length(try))
)

# Plot errors across iterations using ggplot2 after cross-validation
ggplot(df_errors_kfold, aes(x = Iteration, y = Error, color = Model, group = Model)) +
  geom_line() +
  geom_point() +
  labs(title = "K-fold errors in Boosting and Bagging Models over 50 Iterations",
       x = "Iteration",
       y = "Error",
       color = "Model") +
  theme_minimal() +
  theme(legend.position = "top")

# Save the workspace with all the objects
# save.image(file = "kfold_SD.Rdata")